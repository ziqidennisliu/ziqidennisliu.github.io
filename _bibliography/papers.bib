---
---

@inproceedings{10.1145/3706598.3713936,
  author = {Liu, Chang and Wang, Xiangyang and Yu, Chun and Shi, Yingtian and Wang, Chongyang and Liu, Ziqi and Liang, Chen and Shi, Yuanchun},
  title = {Enhancing Smartphone Eye Tracking with Cursor-Based Interactive Implicit Calibration},
  year = {2025},
  isbn = {9798400713941},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi-org.lib.ezproxy.hkust.edu.hk/10.1145/3706598.3713936},
  doi = {10.1145/3706598.3713936},
  abstract = {The limited accuracy of eye-tracking on smartphones restricts its use. Existing RGB-camera-based eye-tracking relies on extensive datasets, which could be enhanced by continuous fine-tuning using calibration data implicitly collected from the interaction. In this context, we propose COMETIC (Cursor Operation Mediated Eye-Tracking Implicit Calibration), which introduces a cursor-based interaction and utilizes the inherent correlation between cursor and eye movement. By filtering valid cursor coordinates as proxies for the ground truth of gaze and fine-tuning the eye-tracking model with corresponding images, COMETIC enhances accuracy during the interaction. Both filtering and fine-tuning use pre-trained models and could be facilitated using personalized, dynamically updated data. Results show COMETIC achieves an average eye-tracking error of 278.3 px (1.60 cm, 2.29Â°), representing a 27.2\% improvement compared to that without fine-tuning. We found that filtering cursor points whose actual distance to gaze is 150.0 px (0.86 cm) yields the best eye-tracking results.},
  booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  articleno = {725},
  numpages = {22},
  keywords = {Eye Tracking, Implicit Calibration, Mobile Devices, Personalization},
  location = {
  },
  series = {CHI '25},
  preview={cometic.png},
  selected={true}
}

@inproceedings{10.1145/3706599.3720290,
  author = {Wang, Yibo and Liu, Ziqi and Xue, Jiao and Lu, Qi},
  title = {AroMR: Decentralizing Olfactory Displays into the Environment for Olfactory-Augmented Experiences in Mixed Reality},
  year = {2025},
  isbn = {9798400713958},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi-org.lib.ezproxy.hkust.edu.hk/10.1145/3706599.3720290},
  doi = {10.1145/3706599.3720290},
  abstract = {Smell, as a vital sensory modality, has the potential to significantly enhance user experience in immersive environments; however, its application in mixed reality (MR) remains underexplored. Current olfactory displays (ODs) are typically fixed to head-mounted displays (HMDs) as active olfactory inputs, limiting interaction and failing to capture the dynamic nature of human olfactory perception. To better integrate olfactory experiences into MR, we propose a "field-centric" strategy that decentralizes ODs into the physical environment as passive outputs. We further introduce AroMR, a proof-of-concept system that employs this strategy to incorporate olfactory feedback into MR experiences. A user study involving eight participants interacting with AroMR across three pre-designed scenarios revealed the potential of our approach to enhance engagement and create more immersive MR experiences. Based on these findings, we propose a conceptual design space to guide future research on integrating olfactory elements into MR.},
  booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  articleno = {75},
  numpages = {7},
  keywords = {Mixed Reality, Olfactory Interface, Interaction Techniques},
  location = {
  },
  series = {CHI EA '25}
  preview={AroMR.png},
  selected={true}
}